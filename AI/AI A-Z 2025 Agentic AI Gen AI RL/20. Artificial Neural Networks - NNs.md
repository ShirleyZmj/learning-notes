# 激活函数（Activation Function）

在深度学习中，**激活函数（Activation Function）**扮演着至关重要的角色。你可以把它想象成神经网络中每个神经元（除了输入层）内部的一个“开关”或者“决策者”。 

**它的主要作用是给神经元引入非线性特性。**

为什么需要非线性呢？

- **处理复杂模式：** 现实世界中的数据和问题往往是非常复杂的，呈现出各种非线性的关系。如果没有激活函数引入非线性，无论神经网络有多少层，每一层都只是前一层输入的线性变换。多层线性变换叠加仍然是线性变换，这样神经网络就无法学习到复杂的非线性模式。
- **模型表达能力：** 非线性激活函数使得神经网络能够逼近任意复杂的函数，这被称为**通用逼近定理（Universal Approximation Theorem）**，是深度学习强大表达能力的基础。

**具体来说，激活函数接收来自上一层神经元的加权输入和偏置项的总和，并将其转换为该神经元的输出。这个输出会作为下一层神经元的输入。**

你可以用一个简单的公式来表示这个过程：

a=g(wTx+b)

其中：

- x 是上一层神经元的输出（当前神经元的输入）
- w 是权重向量
- b 是偏置项
- wTx+b 是神经元的线性组合
- g 是激活函数
- a 是当前神经元的输出

**常见的激活函数有很多，每种都有其特点和适用场景，例如：**

- **Sigmoid 函数：**
    - 公式：σ(x)=1+e−x1​
    - 输出范围：(0, 1)
    - 特点：将输入压缩到0和1之间，可以看作概率输出。早期神经网络中常用，但容易出现梯度消失问题。
- **Tanh (双曲正切) 函数：** Hyperbolic Tangent
    - 公式：tanh(x)=ex+e−xex−e−x​=2σ(2x)−1
    - 输出范围：(-1, 1)
    - 特点：与Sigmoid类似，但输出范围是-1到1，在一定程度上缓解了梯度消失问题。
- **ReLU (Rectified Linear Unit) 函数：**
    - 公式：ReLU(x)=max(0,x)
    - 输出范围：[0,+∞)
    - 特点：当输入大于0时，输出等于输入；当输入小于等于0时，输出为0。计算简单高效，是目前最常用的激活函数之一，有效缓解了梯度消失问题。但可能出现“死亡ReLU”问题（神经元输出始终为0）。
- **Leaky ReLU 函数：**
    - 公式：LeakyReLU(x)={xαx​if x>0if x≤0​，其中 α 是一个很小的正数（例如 0.01）。
    - 输出范围：(−∞,+∞)
    - 特点：解决了ReLU的“死亡ReLU”问题，当输入小于0时，输出一个很小的负数，保持梯度流动。
- **Softmax 函数：**
    - 公式：σ(z)i​=∑j=1K​ezj​ezi​​，其中 z 是一个包含 K 个元素的向量。
    - 输出范围：(0, 1)，且所有元素的和为1。
    - 特点：通常用于多分类问题的输出层，将每个类别的得分转化为概率分布。

选择合适的激活函数对于神经网络的性能至关重要，需要根据具体的任务和网络结构进行选择和调整。

希望这个解释能够帮助你理解深度学习中的激活函数！如果你还有其他问题，随时可以问我。

# 工作原理
![[Pasted image 20250513170740.png]]

没问题！即使你只是全栈工程师，理解神经网络的基本工作原理也并不难。咱们就用你熟悉的“输入-处理-输出”的概念，结合你给的房子价格预测的例子来聊聊。

想象一下，你想要构建一个智能的房价预测工具。你手里有许多房子的数据，每一条数据都包含四个关键信息（输入）：

1. **距离市中心距离：** 房子离市中心有多远（比如，单位是公里）。
2. **房间数量：** 这套房子有几个房间。
3. **房子年龄：** 这房子建了多少年了。
4. **房子面积：** 这房子的使用面积有多大（比如，单位是平方米）。

你的目标是根据这四个输入，预测出这套房子的**总价（输出）**。

**神经网络就像一个非常聪明的“函数”，它能够学习输入和输出之间的复杂关系。它的工作过程可以简化成以下几个步骤：**

**1. 输入层：接收信息**

- 这就像你把房子的四个属性（距离、房间数、年龄、面积）输入到这个智能预测工具里。神经网络的**输入层**就负责接收这四个数值。每个输入属性都对应输入层的一个“神经元”（你可以把它想象成一个小小的信息接收器）。

**2. 隐藏层：学习和提取特征**

- 输入的信息不会直接变成输出，而是会经过一个或多个**隐藏层**。隐藏层是神经网络的核心，它由许多相互连接的**神经元**组成。
- **神经元的工作原理：** 每个隐藏层的神经元都会接收来自上一层（可以是输入层，也可以是另一个隐藏层）所有神经元的输出。它会对这些输入进行**加权求和**，再加上一个**偏置**（你可以理解为微调）。
- **权重和偏置：** 这些**权重**和**偏置**是神经网络在“学习”过程中不断调整的参数。它们决定了哪些输入更重要，以及如何组合这些输入来提取更有意义的**特征**。
    - 举个例子，对于预测房价来说，房子的面积可能比房子年龄更重要，那么与面积相关的权重在学习过程中可能会变得更大。
- **激活函数：引入非线性** 就像我们之前聊到的，每个神经元在加权求和之后，还会通过一个**激活函数**进行处理。这个激活函数的作用是引入非线性，让神经网络能够学习更复杂的模式。你可以暂时把它理解成一个“开关”，决定这个神经元的输出是否重要，以及以什么方式输出。
- **特征提取：** 经过隐藏层的处理，原始的四个输入属性被转化成更抽象、更复杂的“特征”。这些特征可能不再是简单的距离、数量、年龄或面积，而是它们之间某种复杂的组合关系，更能反映房价的决定因素。例如，一个隐藏层的神经元可能学会了识别“高档小区的大户型新房”这种组合特征。

**3. 输出层：给出预测结果**

- 经过一个或多个隐藏层的处理和特征提取，信息最终会传递到**输出层**。
- 对于你的房价预测例子，输出层只有一个神经元，它会根据前面隐藏层学到的特征，最终给出一个**预测的房价数值**。

**4. 学习过程：不断优化**

- 神经网络并不是一开始就能给出准确的预测。它需要通过**学习**大量的历史房价数据来不断调整内部的**权重**和**偏置**。
- **训练数据：** 你会提供大量的房子数据（包含输入属性和实际总价）给神经网络进行训练。
- **预测与比较：** 对于每一条训练数据，神经网络会根据当前的权重和偏置给出一个预测的房价，然后将这个预测值与实际的房价进行比较，计算出**误差**（预测值和真实值之间的差距）。
- **反向传播：调整参数** 神经网络会根据这个误差，反向地调整网络中所有神经元的权重和偏置，使得下一次的预测能够更接近真实值。这个调整的过程就像一个不断试错和优化的过程。
- **迭代学习：** 这个预测和调整的过程会重复很多次，直到神经网络在训练数据上能够给出足够准确的预测。

**总结一下，神经网络预测房价的过程就像这样：**

1. **输入信息：** 输入房子的距离、房间数、年龄和面积。
2. **层层处理：** 这些信息通过多个隐藏层，每个神经元都进行加权求和、添加偏置和激活函数处理，提取出更复杂的特征。
3. **给出预测：** 最终，输出层根据学到的特征给出一个预测的房价。
4. **不断学习：** 通过大量的历史数据进行训练，不断调整网络中的权重和偏置，提高预测的准确性。

希望这个比喻能够帮助你理解神经网络的基本工作原理。虽然背后有很多数学细节，但核心思想就是通过层层的处理和学习，找到输入和输出之间的复杂关系。就像一个黑盒子，你输入信息，它通过内部复杂的计算，最终给你一个有意义的输出。

# 如何学习

调整权重，让加权最小
![[Pasted image 20250513221325.png]]

# 梯度下降 (Gradient Descent)

好的！作为一名全栈工程师，你可以把 **梯度下降 (Gradient Descent)** 想象成一种在复杂地形中寻找最低点的策略。这个“地形”代表着你模型的**损失函数 (Loss Function)**，而最低点就是模型误差最小的地方，也就是模型预测最准确的状态。

在你的房价预测例子中：

- 你的模型（神经网络）会根据输入的四个属性（距离、房间数、年龄、面积）预测一个房价。
- **损失函数**会计算模型预测的房价和实际房价之间的差距（误差）。我们希望这个差距越小越好。

**梯度下降的工作原理就像这样：**

1. **想象你站在山上的某个位置 (模型的初始参数)。** 你看不清山谷（误差最小点）在哪里，但你想尽快下到谷底。
    
2. **环顾四周，找到坡度最陡峭的方向 (梯度)。** 在数学上，**梯度** 指的是函数在某一点变化最快的方向。对于损失函数来说，梯度指向误差增加最快的方向。
    
3. **为了下山，你需要朝着与最陡峭方向相反的方向迈出一步 (负梯度)。** 这就是“下降”的含义。
    
4. **你迈出的一步有多大，取决于你的“学习率 (Learning Rate)”。**
    
    - **学习率太大：** 你可能会一步跨过谷底，在山谷的另一边越走越高。
    - **学习率太小：** 你下山的速度会很慢，需要走很多步才能到达谷底。
5. **重复步骤 2-4，直到你到达一个“谷底” (局部最小值或全局最小值)。** 在这个谷底，无论你朝哪个方向走，坡度都会向上，说明你已经找到了一个误差相对较小的模型参数。
    

**用更贴近神经网络的术语来说：**

- 神经网络的**参数**（比如神经元之间的权重和偏置）就像你在山上的位置。
- **损失函数**衡量了当前参数下，模型预测结果与真实结果之间的差距。
- **梯度下降** 是一种优化算法，它通过计算损失函数相对于模型参数的**梯度**，然后朝着梯度的相反方向（负梯度）更新参数，来逐步减小损失函数的值。
- **学习率** 控制着每次参数更新的步长。

**目标是找到一组模型参数，使得损失函数的值尽可能小，这意味着模型的预测结果与真实结果最接近。**

**需要注意的几点：**

- **局部最小值：** 就像山谷中可能不止一个低洼的地方，损失函数也可能有多个局部最小值。梯度下降可能会陷入一个局部最小值，而无法找到全局的最低点。
- **学习率的选择：** 选择合适的学习率非常重要。过大或过小的学习率都会影响模型的训练效果。

总而言之，梯度下降是一种通过不断调整模型参数，使其在损失函数这个“地形”上朝着“谷底”（误差最小）移动的迭代优化算法。它是训练神经网络的核心方法之一。

## Curse of Dimensionality

好的，全栈工程师！**维度诅咒 (Curse of Dimensionality)** 是机器学习，尤其是在处理高维数据时，经常会遇到的一个令人头疼的问题。你可以把它想象成，当你需要描述一个东西的属性越来越多时，很多原本在低维度空间里表现良好的方法，在高维度空间里会变得非常棘手甚至失效。

回到你的房价预测例子，现在假设我们不仅仅有距离、房间数、年龄和面积这四个属性，还加入了更多的特征，比如：

- 周边学校的评分
- 交通便利程度（地铁站数量、公交线路数量）
- 社区绿化率
- 犯罪率
- 空气质量指数
- 房屋朝向
- 装修程度
- ... 等等，可能有成百上千个特征。

当特征的数量（维度）急剧增加时，就会出现维度诅咒，它会带来一系列负面影响：

**1. 数据稀疏性 (Data Sparsity):**

- 想象一下你有一个固定数量的房子数据。在只有少量特征（比如 4 个）的情况下，这些数据点相对来说可以比较好地覆盖整个特征空间。
- 但是，当特征数量增加到成百上千个时，即使你有大量的数据，这些数据点在高维空间中也会变得非常稀疏。每个数据点周围几乎都是“空白”区域。
- 这就像在一个很大的房间里只放了很少的几个点，这些点之间会显得非常孤立。
- **后果：** 模型很难学习到数据在高维空间中的真实分布，因为很多区域根本没有数据覆盖，导致泛化能力（对新数据的预测能力）下降。

**2. 计算复杂度增加 (Increased Computational Complexity):**

- 许多机器学习算法的计算复杂度会随着数据维度的增加而呈指数级增长。
- 例如，在寻找最近邻时，在高维空间中计算所有数据点之间的距离会变得非常耗时。
- **后果：** 训练模型需要更多的时间和计算资源，甚至变得不可行。

**3. 过拟合风险增加 (Increased Risk of Overfitting):**

- 在高维空间中，模型更容易找到一些看似能够完美拟合训练数据的复杂模式，但这些模式实际上是由于数据的随机性造成的，而不是真实的数据规律。
- 这就像模型记住了训练集中每一个房子的具体价格，但对于没见过的房子就无法做出准确的预测。
- **后果：** 模型在训练集上表现很好，但在新的、未见过的数据上表现很差。

**4. 特征选择变得更加困难 (Difficult Feature Selection):**

- 在高维空间中，识别哪些特征是真正重要的，哪些是冗余或无关的变得更加困难。
- 很多传统的特征选择方法在高维度下可能会失效或者效率低下。
- **后果：** 模型可能会被不重要的特征干扰，降低性能。

**5. 距离度量失效 (Distance Metric Breakdown):**

- 在高维空间中，常用的距离度量（比如欧氏距离）可能会失去意义。
- 由于数据点之间的差异往往体现在非常多的维度上，导致所有数据点之间的距离都变得很大且相似，难以区分。
- **后果：** 基于距离的算法（如 K 近邻）的性能会下降。

**总结来说，维度诅咒是指在高维数据下，数据变得稀疏、计算复杂度增加、过拟合风险增大、特征选择困难以及距离度量失效等一系列问题，这些问题会严重影响机器学习模型的性能。**

**应对维度诅咒的一些常用方法包括：**

- **降维 (Dimensionality Reduction):** 通过一些技术（如主成分分析 PCA、t-SNE 等）将高维数据投影到低维空间，同时尽量保留数据中最重要的信息。
- **特征选择 (Feature Selection):** 从原始特征中选择出最相关的子集。
- **增加数据量 (Increase Data Size):** 在高维空间中，需要更多的数据才能更好地覆盖特征空间。
- **使用对高维数据更鲁棒的算法 (Using Algorithms Robust to High Dimensions):** 有些算法在设计上就考虑了高维数据的特性。
- **正则化 (Regularization):** 限制模型的复杂度，减少过拟合的风险。

希望这个解释能够帮助你理解维度诅咒这个概念！虽然它听起来有点吓人，但却是我们在构建高维数据模型时必须认真对待的问题。


## 随机梯度下降 (Stochastic Gradient Descent, SGD)

作为一种广泛应用于机器学习的优化算法，尤其是训练大规模数据集上的模型，**随机梯度下降 (Stochastic Gradient Descent, SGD)** 是梯度下降算法的一个变体。其核心区别在于每次迭代并非使用全部训练数据来计算梯度，而是**随机选择一个或少量样本**来估计梯度并更新模型参数。

**核心思想：**

与观察整座山的坡度来决定下一步方向的梯度下降不同，随机梯度下降每次**随机地观察山上一小块区域的坡度**，然后朝着这个局部最陡峭的方向迈出一小步。

**具体步骤：**

1.  **初始化参数：** 随机初始化模型的参数。
2.  **迭代训练：** 对于每个训练轮次（epoch）： 
    * **随机打乱数据：** 保证每次迭代采样的随机性。
    * **遍历数据（或小批量）：** 对于训练集中的每个样本（或一个小的样本子集，称为 mini-batch）：
        * **计算梯度：** 使用当前选定的样本（或 mini-batch）计算损失函数关于模型参数的梯度。
        * **更新参数：** 沿着负梯度方向更新模型参数：
            * 单个样本更新：$\theta_{t+1} = \theta_t - \eta \nabla J(\theta_t; x^{(i)}; y^{(i)})$
            * Mini-batch 更新：$\theta_{t+1} = \theta_t - \eta \nabla J(\theta_t; \mathcal{B}^{(t)})$
            其中：
            * $\theta_t$ 是第 $t$ 次迭代的参数。
            * $\eta$ 是学习率。
            * $\nabla J(\theta_t; x^{(i)}; y^{(i)})$ 是使用单个样本计算的梯度。
            * $\nabla J(\theta_t; \mathcal{B}^{(t)})$ 是使用一个 mini-batch 计算的梯度。
3.  **重复迭代：** 直到达到预定的停止条件。

**优点：**

* **计算效率高：** 每次迭代计算量小，尤其适用于大规模数据集。
* **内存需求低：** 可以逐个或分批处理数据。
* **有助于跳出局部最小值：** 随机性有助于探索损失函数曲面。
* **在线学习能力：** 可以处理不断到来的新数据。

**缺点：**

* **收敛过程不稳定：** 损失函数可能出现震荡。
* **需要仔细调整学习率：** 学习率对性能影响大。
* **可能难以精确收敛到全局最小值：** 通常在最小值附近波动。

**Mini-batch Gradient Descent：**

实际应用中更常用 **Mini-batch Gradient Descent**，每次迭代使用一小批样本计算梯度，结合了 SGD 的效率和传统梯度下降的稳定性。

**总结：**

SGD 是一种重要且常用的优化算法，理解其原理对于机器学习模型的训练至关重要。通过合理的参数调整和与其他优化技巧的结合，可以在实践中取得良好的效果。

## Batch Gradient Descent
## 批量梯度下降 (Batch Gradient Descent, BGD)

**批量梯度下降 (Batch Gradient Descent, BGD)** 是一种经典的梯度下降优化算法。与随机梯度下降每次只使用一个或少量样本来计算梯度不同，**BGD 在每次迭代中会使用整个训练数据集来计算损失函数的梯度**，然后根据这个梯度更新模型的所有参数。

**核心思想：**

想象一下你仍然在山上寻找山谷的最低点。批量梯度下降就像你每次都**完整地观察整座山的坡度**，然后朝着整体最陡峭的方向迈出一小步。

**具体步骤：**

1.  **初始化参数：** 随机初始化模型的参数。
2.  **迭代训练：** 对于每个训练轮次（epoch）：
    * **计算梯度：** 使用**整个训练数据集**计算损失函数关于所有模型参数的梯度。
    * **更新参数：** 沿着负梯度的方向更新所有模型参数。更新公式如下：
        $$\theta_{t+1} = \theta_t - \eta \nabla J(\theta_t)$$
        其中：
        * $\theta_t$ 是第 $t$ 次迭代的参数向量。
        * $\eta$ 是学习率。
        * $\nabla J(\theta_t)$ 是使用**整个训练数据集**计算的损失函数 $J$ 关于参数 $\theta_t$ 的梯度。
3.  **重复迭代：** 重复步骤 2 直到达到预定的停止条件。

**优点：**

* **梯度估计准确：** 由于每次迭代都使用整个数据集，因此计算出的梯度是对真实梯度 более точная 估计，收敛方向更稳定。
* **收敛路径平稳：** 损失函数在迭代过程中下降 более плавно，震荡较小。
* **理论保证：** 对于凸优化问题，BGD 保证收敛到全局最小值（在学习率适当的情况下）。

**缺点：**

* **计算成本高：** 每次迭代需要遍历整个数据集，当数据集非常大时，计算量巨大，训练速度非常慢。
* **内存需求高：** 需要将整个数据集加载到内存中进行梯度计算。
* **难以跳出局部最小值：** 对于非凸优化问题，如果陷入局部最小值，由于梯度是基于整个数据集计算的，不太容易跳出去。
* **无法进行在线学习：** 必须在拥有完整数据集后才能进行训练，无法处理动态到来的新数据。

**与随机梯度下降 (SGD) 的对比：**

| 特性           | 批量梯度下降 (BGD)             | 随机梯度下降 (SGD)                 |
| -------------- | ------------------------------ | ---------------------------------- |
| 数据使用       | 整个训练数据集                 | 单个或少量样本 (mini-batch)        |
| 每次迭代计算量 | 大                             | 小                                 |
| 内存需求       | 高                             | 低                                 |
| 收敛速度       | 慢 (每个 epoch)                | 快 (每个 epoch 内多次更新)         |
| 收敛路径       | 平稳，震荡小                   | 波动大，震荡可能较大               |
| 跳出局部最小化 | 较难                           | 相对容易                           |
| 在线学习       | 不支持                         | 支持                               |

**总结：**

批量梯度下降在理论上具有良好的收敛性，但在处理大规模数据集时效率低下。由于其计算成本和内存需求的限制，在现代机器学习中，尤其是在深度学习领域，**SGD 及其变体（如 Mini-batch Gradient Descent）更为常用**。Mini-batch GD 通过在效率和稳定性之间取得平衡，成为了训练大规模模型的首选优化算法之一。

## Backpropagation

好的，我们来聊聊**反向传播 (Backpropagation)**。作为一名全栈工程师，你可能已经听说过这个术语，尤其是在涉及到神经网络和深度学习的时候。反向传播算法是训练人工神经网络的核心算法之一。

**核心思想：**

反向传播本质上是一种**高效计算神经网络中损失函数关于每个权重的梯度**的方法。理解它的关键在于认识到神经网络的复杂性——它由许多层相互连接的神经元组成，每个连接都有一个权重。当我们输入数据并得到一个输出后，我们需要知道这个输出与我们期望的输出之间的差距（即损失），然后调整网络中的权重，使得下一次的输出更接近期望值。

反向传播通过**链式法则**，将输出层的误差逐层地反向传播回网络的每一层，从而计算出每个权重对最终误差的贡献程度（即梯度）。有了这些梯度，我们就可以使用梯度下降（或其变体，比如你之前问到的 SGD）来更新网络的权重，从而最小化损失函数。

**具体步骤（简化描述）：**

1. **前向传播 (Forward Pass)：**
    
    - 将输入数据送入神经网络的输入层。
    - 数据逐层向前传递，每个神经元根据其输入、权重、偏置和激活函数计算输出。
    - 最终在输出层得到网络的预测结果。
    - 根据预测结果和真实标签计算损失函数的值，衡量网络的性能。
2. **反向传播 (Backward Pass)：**
    
    - **计算输出层误差：** 计算输出层的预测值与真实值之间的误差。
    - **反向传播误差：** 从输出层开始，将误差信息沿着网络的连接反向传播回每一层。
    - **计算局部梯度：** 在每个神经元和每个连接上，计算损失函数关于该神经元的输出、输入以及权重的局部梯度。
    - **链式法则应用：** 利用链式法则将后一层的梯度与当前层的局部梯度相乘，从而计算出当前层每个权重对最终损失的偏导数（全局梯度）。
    - **存储梯度：** 将每个权重的梯度存储起来，以便后续的参数更新。
3. **参数更新：**
    
    - 使用计算得到的梯度，结合学习率和优化算法（例如 SGD、Adam），更新网络中的每个权重和偏置，以减小损失函数的值。

**为什么需要反向传播？**

想象一下一个非常深的网络，有成百上千个权重。如果我们想手动计算每个权重对最终损失的影响，那将是极其复杂和低效的。反向传播提供了一种系统化且高效的方法来完成这个任务，它避免了重复计算，使得训练深层神经网络成为可能。

**关键概念：**

- **计算图 (Computational Graph)：** 神经网络的前向传播和反向传播过程可以看作是在一个计算图上进行的操作。节点表示操作（例如，加法、乘法、激活函数），边表示数据流。
- **链式法则 (Chain Rule)：** 这是反向传播的核心数学原理。如果 y=f(u) 且 u=g(x)，那么 dxdy​=dudy​⋅dxdu​。在神经网络中，每一层的输出都是下一层的输入，通过链式法则可以将误差梯度逐层传递。
- **局部梯度 (Local Gradient)：** 指的是某个神经元或某个操作的输出关于其输入的梯度。
- **全局梯度 (Global Gradient)：** 指的是网络最终的损失函数关于某个参数（权重或偏置）的梯度，这是我们最终用于参数更新的值。

**总结：**

反向传播是一种强大而高效的算法，它使得神经网络能够从数据中学习并不断改进其性能。作为一名全栈工程师，理解反向传播的原理可以帮助你更好地理解深度学习框架的工作方式，更有效地调试和优化你的模型。虽然你可能不需要从头开始实现反向传播（因为现代深度学习框架已经封装好了这些细节），但对其基本原理的理解对于深入掌握深度学习至关重要。

希望这个解释对你有帮助！如果你有更具体的问题，欢迎继续提问。 


## Training the ANN with Stochastic Gradient Descent

### Original English Text:
STEP 1: Randomly initialise the weights to small numbers close to 0 (but not 0).
STEP 2: Input the first observation of your dataset in the input layer, each feature in one input node.
STEP 3: Forward-Propagation: from left to right, the neurons are activated in a way that the impact of each neuron's activation is limited by the weights. Propagate the activations until getting the predicted result y.
STEP 4: Compare the predicted result to the actual result. Measure the generated error.
STEP 5: Back-Propagation: from right to left, the error is back-propagated. Update the weights according to how much they are responsible for the error. The learning rate decides by how much we update the weights.
STEP 6: Repeat Steps 1 to 5 and update the weights after each observation (Reinforcement Learning). Or: Repeat Steps 1 to 5 but update the weights only after a batch of observations (Batch Learning).
STEP 7: When the whole training set passed through the ANN, that makes an epoch. Redo more epochs.

### 使用随机梯度下降训练人工神经网络 (ANN)

步骤 1: 将权重随机初始化为接近 0 的小数 (但不能是 0)。
步骤 2: 将数据集中的第一个观测值输入到输入层，每个特征对应一个输入节点。
步骤 3: 前向传播：从左到右，神经元被激活，每个神经元激活的影响受权重限制。传播激活直到获得预测结果 y。
步骤 4: 将预测结果与实际结果进行比较。衡量产生的误差。
步骤 5: 反向传播：从右到左，误差被反向传播。根据权重对误差的“贡献”程度来更新权重。学习率决定了我们更新权重的幅度。
步骤 6: 重复步骤 1 到 5，并在每次观测后更新权重 (强化学习)。或者：重复步骤 1 到 5，但仅在一批观测之后更新权重 (批量学习)。
步骤 7: 当整个训练集通过人工神经网络后，这就完成了一个周期 (epoch)。重复进行更多的周期。
深度学习 A-Z

### 解释 (Explanation):
这段文字描述了使用随机梯度下降 (Stochastic Gradient Descent, SGD) 算法训练人工神经网络 (Artificial Neural Network, ANN) 的步骤。这是一个在机器学习和深度学习中非常核心和常见的训练方法。
 * 核心思想 (Core Idea): 神经网络通过学习数据中的模式来进行预测或分类。训练过程就是调整网络内部的参数（称为“权重”），以最小化预测结果与实际结果之间的误差。
 * 步骤分解 (Step-by-Step Breakdown):
   * 初始化权重 (Step 1: Initialization): 训练开始前，网络中的连接权重被赋予随机的初始值。这些值通常很小且接近于零，但不完全是零，以避免计算上的问题并帮助打破对称性。
   * 输入数据 (Step 2: Input Data): 将训练数据集中的一个样本（或一批样本中的第一个）输入到网络的输入层。每个输入特征对应输入层的一个神经元节点。
   * 前向传播 (Step 3: Forward-Propagation):
     * 数据从输入层开始，逐层向前传递到输出层。
     * 在每一层，神经元接收来自前一层的加权输入。
     * 这些加权输入通过一个“激活函数”处理，产生该神经元的输出。
     * 这个过程一直持续到输出层，最终产生一个预测结果 (y)。
     * “权重”在这个过程中起着关键作用，它们决定了每个连接对下一层神经元激活程度的影响。
   * 计算误差 (Step 4: Error Calculation): 将神经网络的预测结果与该样本的真实标签（实际结果）进行比较，计算两者之间的差异，即“误差”或“损失”。这个误差衡量了网络预测的准确性。
   * 反向传播 (Step 5: Back-Propagation):
     * 这是训练过程的核心。误差从输出层开始，反向传播回网络的每一层。
     * 在反向传播过程中，会计算每个权重对总误差的“贡献”程度（即梯度）。
     * 然后，根据这些梯度和预设的“学习率 (learning rate)”来调整权重。学习率是一个超参数，控制着每次更新权重的大小。目标是朝着减小误差的方向调整权重。
   * 迭代更新 (Step 6: Iteration and Weight Update):
     * 随机梯度下降 (Stochastic Gradient Descent - Implied by "after each observation"): 在标准的SGD中，每处理一个训练样本（观测值），就执行一次前向传播、误差计算、反向传播和权重更新。这被称为“在线学习”或有时与“强化学习”中的概念相联系（尽管这里的“强化学习”用法可能不完全标准，通常SGD是指在单个样本或小批量样本上更新）。
     * 批量学习 (Batch Learning / Mini-Batch Gradient Descent): 或者，可以先处理一小批 (mini-batch) 训练样本，计算这批样本的平均误差，然后根据这个平均误差进行一次权重更新。如果批次大小等于整个数据集，则称为“批量梯度下降 (Batch Gradient Descent)”。
     * 图片中提到的“Reinforcement Learning”在这里可能指的是每处理一个样本就立即根据反馈（误差）调整策略（权重），这与强化学习中智能体与环境交互并根据奖励/惩罚调整行为有相似之处。但更标准的术语是逐样本更新。
   * 周期 (Epochs) (Step 7: Epochs):
     * 当整个训练数据集中的所有样本都通过神经网络（即完成了前向传播、反向传播和权重更新）一次后，就称为完成了一个“周期 (epoch)”。
     * 通常需要多个周期才能使神经网络充分学习数据中的模式并达到较好的性能。因此，会重复整个过程（步骤1-6）多次。
 * “to allowing your neural network to train better” / “到让你的神经网络更好地训练”: 这句话总结了上述所有步骤的最终目的，即通过这个迭代优化的过程，让神经网络模型能够更有效地从数据中学习，从而提高其预测或分类的准确性。
总而言之，这个过程就像教一个学生：给他看一个问题（输入数据），让他给出答案（前向传播），然后告诉他答案错在哪里以及错的程度（计算误差），接着指导他如何改进思考方式以避免下次犯同样的错误（反向传播和权重更新），不断重复这个过程（多个周期和样本），直到他能很好地解答这类问题。
