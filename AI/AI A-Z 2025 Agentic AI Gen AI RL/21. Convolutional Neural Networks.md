# 1. Convolution
1. Input Image
2. Feature Detector / Filter
	- Edge Enhance
	- Blur
	- Sharpen
	- Emboss
### Feature Detector / Filter

以下是一些常见的图像处理滤波器及其解释和示例矩阵值（3x3）：

**1. Edge Enhance (边缘增强)**

* **解释:** 边缘增强滤波器的目的是突出图像中颜色或强度变化剧烈的区域，即边缘。它可以使图像的边缘看起来更清晰。

* **示例矩阵值:**
    ```
    0  -1  0
   -1   5 -1
    0  -1  0
    ```
    或
    ```
   -1  -1  -1
   -1   9  -1
   -1  -1  -1
    ```

**2. Blur (模糊)**

* **解释:** 模糊滤波器通过平均或加权平均邻近像素的值来减少图像中的细节和噪声。它可以使图像看起来更平滑。常见的模糊滤波器包括均值模糊和高斯模糊。

* **示例矩阵值 (均值模糊 - Average Blur):**
    ```
    1/9 1/9 1/9
    1/9 1/9 1/9
    1/9 1/9 1/9
    ```

* **示例矩阵值 (高斯模糊 - Gaussian Blur，近似):**
    ```
    1/16 2/16 1/16
    2/16 4/16 2/16
    1/16 2/16 1/16
    ```

**3. Sharpen (锐化)**

* **解释:** 锐化滤波器与模糊相反，它的目的是增强图像中的细节，使图像看起来更清晰锐利。它通常通过突出像素及其周围像素之间的差异来实现。

* **示例矩阵值:**
    ```
    0  -1  0
   -1   5 -1
    0  -1  0
    ```
    （与一种边缘增强滤波器相同，但其效果也包含锐化）
    或
    ```
   -1  -1  -1
   -1   9  -1
   -1  -1  -1
    ```
    （也具有锐化效果）
    或
    ```
    -1 -1 -1
    -1  8 -1
    -1 -1 -1
    ```

**4. Emboss (浮雕)**

* **解释:** 浮雕滤波器通过突出图像中具有特定方向的亮度差异，产生一种三维浮雕效果。它会使图像看起来像是由凸起和凹陷组成的。

* **示例矩阵值:**
    ```
    -2 -1  0
    -1  1  1
     0  1  2
    ```
    或
    ```
    -1 -1  0
    -1  0  1
     0  1  1
    ```

**请注意:**

* 这些矩阵是 **示例**，实际应用中可能会使用不同大小和值的矩阵来达到不同的效果。
* 滤波器的大小不限于 3x3，常见的还有 5x5、7x7 等。
* 对于模糊滤波器，高斯模糊通常比均值模糊效果更自然。
* 锐化滤波器有时会增强图像中的噪声，因此需要谨慎使用。
* 浮雕滤波器的效果取决于矩阵中正负值的排列。

1. Feature Map

## Step1 - ReLU in CNN: Understanding Non-Linearity for Deep 


---

# 2. Max Pooling
## Step 2 - Max Pooling in CNN: How to Reduce Feature Maps & Prevent Overfitting

http://scs.ryerson.ca/~aharley/vis/conv/flat.html

![[Pasted image 20250514202300.png]]


--- 
# 3. Flattening
## Step 3 - CNN Feature Map Flattening: From Pooling Layer to Neural Network Input

![[Pasted image 20250514202639.png]]
![[Pasted image 20250514202743.png]]

---
# 4. Full Connection
 ![[Pasted image 20250514204321.png]]

![[Pasted image 20250514204343.png]]

# Softmax and Cross-Entropy Loss Functions in Neural Networks

好的，作为一名全栈工程师，你可以将交叉熵（Cross-Entropy）理解为一个**衡量“实际情况”与“你的预测”之间差异的指标**。特别是在机器学习领域，它扮演着非常重要的角色，尤其是在分类问题中。

想象一下，你正在做一个项目，需要预测用户会点击哪个按钮：“购买”还是“稍后提醒”。

* **实际情况 (Ground Truth):** 用户最终点击了“购买”。我们可以用一个概率分布来表示这个情况，比如：`[购买: 1, 稍后提醒: 0]`。这表示100%的概率是“购买”。
* **你的预测 (Model's Prediction):** 你的系统（比如一个机器学习模型）预测用户有70%的概率点击“购买”，30%的概率点击“稍后提醒”。这也可以用一个概率分布表示：`[购买: 0.7, 稍后提醒: 0.3]`。

**交叉熵的作用就是量化这两个概率分布之间的“距离”或“不相似度”。**

* 如果你的预测非常接近实际情况（例如，预测99%是“购买”，1%是“稍后提醒”），那么交叉熵的值会很小。
* 如果你的预测与实际情况偏差很大（例如，预测10%是“购买”，90%是“稍后提醒”），那么交叉熵的值会很大。

### 为什么这对全栈工程师来说可能有用？

虽然你可能不直接编写交叉熵的数学公式，但在以下场景中，理解其概念会很有帮助：

1.  **理解机器学习模型的工作原理：** 当你使用或集成一个机器学习模型（例如，用于推荐系统、图像识别、自然语言处理）时，这些模型在“学习”阶段通常会使用一个叫做**损失函数 (Loss Function)** 的东西来评估自己预测的好坏。交叉熵就是一种非常常用的损失函数，特别是在分类任务中。模型的目标就是通过调整自身参数来最小化这个损失函数（也就是最小化交叉熵的值），从而让预测越来越接近实际。
2.  **评估模型性能：** 虽然准确率 (Accuracy) 是一个直观的指标，但交叉熵能提供更细致的关于模型预测概率质量的信息。一个低交叉熵的模型通常意味着它不仅能做出正确的分类，而且对正确的分类抱有较高的置信度。
3.  **与数据科学家/机器学习工程师协作：** 如果你与数据科学家或机器学习工程师一起工作，他们可能会讨论模型的损失函数或交叉熵。理解这个概念能帮助你更好地与他们沟通。
4.  **选择或配置AI服务：** 当你使用云服务提供商（如AWS, Google Cloud, Azure）提供的AI服务时，了解一些基本概念如交叉熵，可以帮助你更好地理解服务的能力和配置选项。

### 简单类比（不完全精确，但有助于理解）：

想象你在玩一个猜数字游戏，范围是1到100。

* **实际数字：** 73
* **你的猜测1：** 70。 你离得比较近。
* **你的猜测2：** 10。 你离得很远。

交叉熵有点像衡量你猜测的“离谱程度”，但它处理的是概率分布，而不是单个数值的远近。

### 在技术层面：

交叉熵来源于信息论。它的数学公式是：

$H(p, q) = - \sum_x p(x) \log q(x)$

其中：

* $p(x)$ 是真实概率分布（比如用户实际点击了哪个按钮）。
* $q(x)$ 是你的模型预测的概率分布。

对于二分类问题（比如“购买”或“稍后提醒”），这个公式会简化。如果实际是类别1，预测是类别1的概率为 $y'$, 那么交叉熵就是 $- \log(y')$。如果预测的 $y'$ 越接近1（真实情况），那么 $- \log(y')$ 就越接近0（损失小）。如果 $y'$ 越接近0，那么 $- \log(y')$ 就越大（损失大）。
![[Pasted image 20250514213122.png]]

### 总结一下，对于全栈工程师而言，你可以将交叉熵理解为：

* **一个衡量标准：** 用来衡量你的预测模型输出的概率与真实结果的概率之间的差异。
* **一个优化目标：** 在机器学习训练中，模型会努力最小化交叉熵损失，从而使预测更准确、更自信。
* **一个沟通桥梁：** 帮助你理解和讨论机器学习模型的核心工作机制。

希望这个解释对你有帮助！
