## Step 1: Installing and importing the libraries

```
!pip uninstall accelerate peft bitsandbytes transformers trl -y

!pip install accelerate peft==0.13.2 bitsandbytes transformers trl==0.12.0
```

这两行代码是在 Google Colab 中使用 `pip` 命令来管理 Python 库。它们的作用是先卸载一些库，然后再安装特定版本的库。

**第一行代码：**

```
!pip uninstall accelerate peft bitsandbytes transformers trl -y
```

[Use code with caution](https://g.co/legal/generative-code)

- `!pip uninstall`: 这是 `pip` 命令的卸载功能，用于卸载指定的 Python 库。
- `accelerate peft bitsandbytes transformers trl`: 这些是要卸载的库的名称。这些库通常用于机器学习和深度学习，特别是与 Hugging Face Transformers 相关的任务。
- `-y`: 这个参数表示自动确认卸载操作，无需手动输入确认信息。

**第二行代码：**

```
!pip install accelerate peft==0.13.2 bitsandbytes transformers trl==0.12.0
```

[Use code with caution](https://g.co/legal/generative-code)

- `!pip install`: 这是 `pip` 命令的安装功能，用于安装指定的 Python 库。
- `accelerate peft==0.13.2 bitsandbytes transformers trl==0.12.0`: 这些是要安装的库的名称和版本号。
    - `peft==0.13.2` 和 `trl==0.12.0`：表示要安装 `peft` 库的 0.13.2 版本和 `trl` 库的 0.12.0 版本。指定版本号可以确保安装特定版本的库，以避免代码兼容性问题。
    - `accelerate` 和 `bitsandbytes` : 表示要安装这两个库，没有指定版本号，安装最新版本

**总结：**

这两行代码的作用是：

1. 卸载 `accelerate`、`peft`、`bitsandbytes`、`transformers` 和 `trl` 这几个库，以确保安装干净的指定版本。
2. 安装 `accelerate` 库的最新版本， `peft` 库的 0.13.2 版本，`bitsandbytes` 库的最新版本, `transformers` 库的最新版本以及 `trl` 库的 0.12.0 版本。

这样做是为了确保你的环境中安装了正确版本的库，以便你的代码能够正常运行。在机器学习项目中，库的版本兼容性非常重要，使用 `pip` 命令来管理库可以帮助你更好地控制你的环境。

## Step 2: Loading the model

```
llama_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path = "aboonaji/llama2finetune-v2",

quantization_config = BitsAndBytesConfig(load_in_4bit = True, bnb_4bit_compute_dtype = getattr(torch, "float16"), bnb_4bit_quant_type = "nf4"))

llama_model.config.use_cache = False

llama_model.config.pretraining_tp = 1
```

这段代码使用 `transformers` 库加载了一个预训练的 Llama 2 模型，并应用了 4 位量化技术来减少内存使用。以下是分步说明：

1. **导入必要的库：** 在运行此代码之前，请确保已安装 `transformers` 和 `bitsandbytes` 库。您可以使用以下命令安装它们：

```
!pip install transformers bitsandbytes
```

[Use code with caution](https://g.co/legal/generative-code)

2. **使用量化加载预训练模型：**

```
llama_model = AutoModelForCausalLM.from_pretrained(       pretrained_model_name_or_path="aboonaji/llama2finetune-v2",       quantization_config=BitsAndBytesConfig(           load_in_4bit=True,           bnb_4bit_compute_dtype=getattr(torch, "float16"),           bnb_4bit_quant_type="nf4"       )   )
```

[Use code with caution](https://g.co/legal/generative-code)

- `AutoModelForCausalLM.from_pretrained`：此函数加载用于因果语言建模的预训练模型。
- `pretrained_model_name_or_path="aboonaji/llama2finetune-v2"`：这指定了要从 Hugging Face 模型中心加载的预训练模型的名称或路径。在本例中，它是 "aboonaji/llama2finetune-v2"。
- `quantization_config`：此参数使用 `BitsAndBytesConfig` 配置模型的量化。
    - `load_in_4bit=True`：以 4 位精度加载模型以减少内存使用量。 这意味着模型的权重将以更低的精度存储，从而减少内存占用。
    - `bnb_4bit_compute_dtype=getattr(torch, "float16")`：指定量化期间计算的数据类型（在本例中为 float16）。 这指定了在进行计算时使用的精度，float16 是一种比 float32 更低精度的浮点数类型。
    - `bnb_4bit_quant_type="nf4"`：将量化类型设置为 NF4。 这指定了使用的特定量化方法。NF4 是一种常见的 4 位量化技术。

3. **配置模型设置：**

```
llama_model.config.use_cache = False   llama_model.config.pretraining_tp = 1
```

[Use code with caution](https://g.co/legal/generative-code)

- `llama_model.config.use_cache = False`：禁用模型的缓存。 这可以节省内存，但可能会稍微降低推理速度。
- `llama_model.config.pretraining_tp = 1`：在预训练期间将张量并行度设置为 1。 这指定了在预训练模型时使用的张量并行度。

**总结：**

这段代码加载了一个预训练的 Llama 2 模型，并应用了 4 位量化技术和一些配置来减少内存使用和提高效率。这些设置在资源有限的环境（如 Google Colab）中尤其有用，因为它允许您更有效地加载和使用大型语言模型。

## Step 3: Loading the tokenizer

```
llama_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path = "aboonaji/llama2finetune-v2", trust_remote_code = True)

llama_tokenizer.pad_token = llama_tokenizer.eos_token

llama_tokenizer.padding_side = "right"
```

这段代码使用 `transformers` 库加载了一个预训练的 Llama 2 模型的分词器，并进行了一些配置。以下是分步说明：

1. **加载预训练模型的分词器：**

```
llama_tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path = "aboonaji/llama2finetune-v2", trust_remote_code = True)
```

[Use code with caution](https://g.co/legal/generative-code)

- `AutoTokenizer.from_pretrained`：此函数加载预训练模型的分词器。分词器是将文本转换为模型可以理解的数字表示的工具。
- `pretrained_model_name_or_path = "aboonaji/llama2finetune-v2"`：这指定了要加载的分词器的预训练模型的名称或路径。在本例中，它是 "aboonaji/llama2finetune-v2"，与之前加载的 Llama 2 模型相同。
- `trust_remote_code = True`：这表示信任远程代码，因为某些分词器可能需要执行远程代码来加载。

2. **设置填充标记：**

```
llama_tokenizer.pad_token = llama_tokenizer.eos_token
```

[Use code with caution](https://g.co/legal/generative-code)

- `llama_tokenizer.pad_token`：这设置了分词器的填充标记。填充标记用于将不同长度的序列填充到相同的长度，以便模型可以处理它们。
- `llama_tokenizer.eos_token`：这表示分词器的结束标记。在本例中，我们将填充标记设置为结束标记。这意味着当需要填充序列时，将使用结束标记进行填充。

3. **设置填充方向：**

```
llama_tokenizer.padding_side = "right"
```

[Use code with caution](https://g.co/legal/generative-code)

- `llama_tokenizer.padding_side`：这设置了分词器的填充方向。
- `"right"`：这表示将在序列的右侧进行填充。这意味着填充标记将添加到序列的末尾。

**总结：**

这段代码加载了一个预训练的 Llama 2 模型的分词器，并将填充标记设置为结束标记，并将填充方向设置为右侧。这些设置是为了确保模型能够正确处理不同长度的序列。

**更详细的解释：**

- **分词器:** 分词器是自然语言处理中的一个重要工具，它将文本分解成称为“标记”的较小单元。这些标记可以是单词、子词或字符，具体取决于分词器的类型。模型使用这些标记来理解文本的含义。
- **填充:** 在处理多个文本序列时，它们的长度通常不同。为了使模型能够有效地处理这些序列，我们需要将它们填充到相同的长度。填充是通过添加特殊的填充标记来实现的。
- **填充方向:** 填充可以在序列的左侧或右侧进行。在本例中，我们选择在右侧进行填充，这意味着填充标记将添加到序列的末尾。

## 4. Setting the training arguments
```
training_arguments = TrainingArguments(output_dir = "./results", per_device_train_batch_size = 4, max_steps = 100)
```
这段代码是用 `transformers` 库中的 `TrainingArguments` 类来定义训练参数。这些参数将会被用于微调语言模型。

让我们来分解一下各个参数的含义：

1. `output_dir = "./results"`: 这个参数指定了模型训练结果的保存路径，这里是保存在当前目录下的 "results" 文件夹中。
2. `per_device_train_batch_size = 4`: 这个参数指定了每个设备（例如GPU）上用于训练的批次大小，这里是 4。这意味着每次训练会将 4 个样本作为一个批次输入模型。
3. `max_steps = 100`: 这个参数指定了模型训练的最大步数，这里是 100 步。模型会在训练 100 步后停止，无论是否达到最佳性能。

总的来说，这段代码定义了模型训练的一些基本参数，包括结果保存路径、批次大小和训练步数。这些参数会影响模型的训练速度、性能和最终结果。

## 5.  Creating the Supervised Fine-Tuning trainer

```
llama_sft_trainer = SFTTrainer(model = llama_model,

args = training_arguments,

train_dataset = load_dataset(path = "aboonaji/wiki_medical_terms_llam2_format", split = "train"),

tokenizer = llama_tokenizer,

peft_config = LoraConfig(task_type = "CAUSAL_LM", r = 64, lora_alpha = 16, lora_dropout = 0.1),

dataset_text_field = "text")
```

这段代码使用 `trl` 库中的 `SFTTrainer` 类创建了一个用于监督式微调 (SFT) 的训练器。这个训练器将会使用你之前定义的模型、训练参数和数据集来微调你的 LLaMa 模型。

让我们来分解一下各个参数的含义：

1. `model = llama_model`: 这个参数指定了要进行微调的模型，这里是之前加载的 `llama_model`。
2. `args = training_arguments`: 这个参数指定了训练参数，这里是之前定义的 `training_arguments`。
3. `train_dataset = load_dataset(path = "aboonaji/wiki_medical_terms_llam2_format", split = "train")`: 这个参数指定了用于训练的数据集，这里是使用 `datasets` 库加载的 "aboonaji/wiki_medical_terms_llam2_format" 数据集的 "train" 部分。这个数据集包含了医学术语和相关信息，用于微调模型以更好地理解和生成医学文本。
4. `tokenizer = llama_tokenizer`: 这个参数指定了用于处理文本的分词器，这里是之前加载的 `llama_tokenizer`。
5. `peft_config = LoraConfig(task_type = "CAUSAL_LM", r = 64, lora_alpha = 16, lora_dropout = 0.1)`: 这个参数指定了使用 LoRA (Low-Rank Adaptation) 技术进行微调的配置。LoRA 是一种高效的微调方法，它通过向模型添加少量可训练参数来提高性能。
    - `task_type = "CAUSAL_LM"`: 指定了任务类型为因果语言建模，这意味着模型将被训练用于生成文本。
    - `r = 64`: 指定了 LoRA 的秩，它控制了添加到模型中的可训练参数的数量。
    - `lora_alpha = 16`: 指定了 LoRA 的 alpha 值，它控制了 LoRA 参数对模型输出的影响程度。
    - `lora_dropout = 0.1`: 指定了 LoRA 的 dropout 率，它是一种正则化技术，用于防止模型过拟合。
6. `dataset_text_field = "text"`: 这个参数指定了数据集中包含文本内容的字段名称，这里是 "text"。这意味着训练器将使用数据集中的 "text" 字段作为模型的输入。

总的来说，这段代码创建了一个用于微调 LLaMa 模型的训练器，并指定了模型、训练参数、数据集、分词器和 LoRA 配置。这个训练器将使用医学术语数据集对模型进行微调，使其能够更好地理解和生成医学文本。

### Token
目前市面上很多语言模型的收费都是按照 **token** 数量来计算的。

**Token** 是语言模型处理文本的基本单位，可以是一个单词、一个字符或者一个子词。例如，"hello world" 这个短语可以被分成 "hello" 和 "world" 两个 token。

**为什么按 token 收费？**

- **计算成本：** 处理 token 是语言模型的主要计算任务，token 数量越多，计算量就越大，成本也就越高。
- **资源消耗：** 语言模型需要存储大量的参数和数据，token 数量越多，占用的存储空间和内存也就越大，资源消耗也就越高。
- **模型复杂度：** 更复杂的模型通常需要处理更多的 token，因此收费也更高。

## Step 6: Training the model

```
llama_sft_trainer.train()
```

`llama_sft_trainer.train()` 这行代码的作用是 **开始训练** 你之前定义的 `llama_sft_trainer`。

在这个过程中，训练器会使用你指定的训练数据集、模型、训练参数和 LoRA 配置来微调你的 LLaMa 模型。

**训练过程会发生什么？**

1. **数据加载：** 训练器会加载你指定的训练数据集，并将数据分成多个批次。
2. **前向传播：** 训练器会将每个批次的数据输入模型，并计算模型的输出。
3. **损失计算：** 训练器会将模型的输出与实际目标值进行比较，并计算损失函数。损失函数衡量了模型预测结果与实际结果之间的差距。
4. **反向传播：** 训练器会根据损失函数的值，计算模型参数的梯度，并使用梯度下降等优化算法更新模型参数。
5. **重复迭代：** 训练器会重复步骤 2-4，直到达到指定的训练步数或者模型性能达到预期目标。

**训练完成后，你的 LLaMa 模型将会被微调，并保存到 `training_arguments` 中指定的 `output_dir` 路径下。**

**注意：**

- 训练过程可能需要一段时间，具体时间取决于你的数据集大小、模型复杂度、训练参数和硬件配置等因素。
- 你可以使用 `training_arguments` 中的其他参数来控制训练过程，例如 `logging_dir` 用于指定日志保存路径，`evaluation_strategy` 用于指定评估策略等。

## Step 7: Chatting with the model

```
user_prompt = "Please tell me about Bursitis"

text_generation_pipeline = pipeline(task = "text-generation", model = llama_model, tokenizer = llama_tokenizer, max_length = 500)

model_answer = text_generation_pipeline(f"<s>[INST] {user_prompt} [/INST]")

print(model_answer[0]['generated_text'])
```

这段代码的作用是使用你微调过的 LLaMa 模型来生成对用户提示的响应。

以下是逐行解释：

1. **`user_prompt = "Please tell me about Bursitis"`**: 这行代码定义了你想要给模型的提示。在这里，你要求模型告诉你关于滑囊炎的信息。
2. **`text_generation_pipeline = pipeline(task = "text-generation", model = llama_model, tokenizer = llama_tokenizer, max_length = 500)`**: 这行代码使用 Hugging Face 的 `pipeline` 函数创建了一个文本生成管道。
    - `task = "text-generation"` 指定你想要使用这个管道进行文本生成。
    - `model = llama_model` 使用你微调过的 LLaMa 模型来生成文本。
    - `tokenizer = llama_tokenizer` 使用相应的tokenizer来处理文本。
    - `max_length = 500` 将生成的文本的最大长度设置为 500 个token。
3. **`model_answer = text_generation_pipeline(f"<s>[INST] {user_prompt} [/INST]")`**: 这行代码使用管道来生成对你的提示的响应。
    - 提示的格式是 `<s>[INST] {user_prompt} [/INST]`，这是一种指示语言模型的常用格式。`<s>` 表示序列开始标记，`[INST]` 表示指令，`[/INST]` 标记指令的结束。
4. **`print(model_answer[0]['generated_text'])`**: 这行代码打印模型响应中生成的文本。 `model_answer` 是一个包含管道输出的列表，`[0]['generated_text']` 从列表的第一个元素访问生成的文本。

**总而言之，这段代码接收用户提示，使用文本生成管道将其输入到你微调过的 LLaMa 模型，然后打印模型生成的响应。**


